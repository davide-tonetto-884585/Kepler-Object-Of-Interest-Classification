---
title: "KOI analysis - Conclusions"
author: "Davide Tonetto"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    theme: cosmo          
    highlight: tango      
    toc: true             
    toc_float: true       
    toc_depth: 3          
    number_sections: true 
    df_print: paged       
    code_folding: show    
    fig_width: 10         
    fig_height: 6         
---

```{r setup, include=FALSE}
# Set CRAN mirror
options(repos = c(CRAN = "https://cloud.r-project.org"))

# Function to install and load required packages
packages <- c("tidyverse")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(!installed_packages)) {
  install.packages(packages[!installed_packages])
}

# Load all required packages
invisible(lapply(packages, library, character.only = TRUE))

# Set global chunk options
knitr::opts_chunk$set(
  warning = FALSE, # Don't show warnings in the output
  message = FALSE, # Don't show package loading messages
  echo = TRUE, # Show R code chunks in the output
  fig.width = 10, # Set default figure width
  fig.height = 6 # Set default figure height
)
```

# Models comparison
## Load models performance from Rda files
```{r}
# Load models performance from Rda files
load("data/Rdas/models_performance.Rda")
load("data/Rdas/models_performance_pca.Rda")

final_models_performance <- bind_rows(models_performance, models_performance_pca)
final_models_performance
```

plot models performance
```{r}
# Create long format data for plotting
performance_long <- final_models_performance %>%
  pivot_longer(
    cols = c(Accuracy, Sensitivity, Specificity, AUC),
    names_to = "Metric",
    values_to = "Value"
  )

# Create plots
ggplot(performance_long, aes(x = Model, y = Value, fill = Metric)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5, size = 16),
    legend.position = "top"
  ) +
  labs(
    title = "Model Performance Comparison",
    x = "Models",
    y = "Score"
  ) +
  ylim(0, 1)
```

# Conclusion: Model Selection for KOI Analysis
This analysis aimed to determine the most effective model for classifying Kepler Objects of Interest (KOIs) as either "CANDIDATE" exoplanets or "FALSE POSITIVEs". Several modeling approaches were explored, including Generalized Linear Models (GLM), Generalized Additive Models (GAM), GLM with interaction terms, Lasso Regression, and Ridge Regression. The performance of these models was evaluated based on Accuracy, Sensitivity, Specificity, and Area Under the ROC Curve (AUC).

## Key Findings
**Original Features vs. PCA Features:** Models trained directly on the original (scaled) features generally demonstrated superior performance compared to those trained on PCA features in this analysis. The GLM, GAM, Lasso, and Ridge models using original features all achieved AUCs above 0.88, whereas the models using PCA features had lower AUCs (0.70-0.87) and often exhibited extreme trade-offs between Sensitivity and Specificity, potentially influenced by the definition of the positive class during evaluation. This suggests that PCA, as implemented here, might have resulted in some information loss detrimental to predictive performance for this specific task compared to using the original, interpretable features.

## Interpretation and Model Efficacy
The initial **GLM** provided a foundational understanding, identifying several significant predictors for exoplanet disposition. However, residual analysis indicated that linear relationships were insufficient for many variables, suggesting the need for more complex models. Iterations on the GLM involved outlier removal and handling multicollinearity by removing variables like `koi_sma` and `koi_ldm_coeff2`, which led to an improved AIC and better model fit compared to the initial GLM. Key predictors consistently associated with "FALSE POSITIVE" status in the refined GLM included `koi_duration`, `koi_depth`, `koi_teq`, `koi_slogg`, `koi_smass`, `koi_impact`, `koi_ror`, and `koi_dor`. Conversely, higher `koi_insol`, `koi_steff` (though this flipped in the GAM), `koi_incl`, and `koi_smet` were associated with a higher likelihood of being a "CANDIDATE".

The **Generalized Additive Model (GAM)** demonstrated superior performance across all key metrics, achieving the highest AUC (0.9590), Accuracy (0.8926), Sensitivity (0.8899), and Specificity (0.8958). This improvement can be attributed to its ability to capture non-linear relationships for critical predictors such as `koi_teq`, `koi_slogg`, `koi_dor`, and `koi_smet`, as evidenced by their significant smooth terms and estimated degrees of freedom (edf) greater than one. The GAM interpretation highlighted that the influence of these variables is more complex than a simple linear trend. Notably, the planet-to-star radius ratio (`koi_ror`) remained an overwhelmingly strong linear predictor of "FALSE POSITIVEs" in the GAM, and the effect of stellar effective temperature (`koi_steff`) shifted, increasing the likelihood of a false positive when non-linearities in other variables were accounted for.

The **GLM with Interaction Terms** showed a slight improvement in AUC over the basic GLM but was marked by severe numerical problems during fitting, rendering its detailed coefficient interpretation unreliable. This suggests that while interactions might be present, this particular approach struggled with model stability and interpretability due to complexity.

**Lasso Regression** performed feature selection, shrinking several coefficients to zero. Key predictors for "FALSE POSITIVE" included `koi_ror`, `koi_teq`, and `koi_depth`, while `koi_incl` and `koi_smet` were important for "CANDIDATE" status. Its performance was comparable to the refined GLM but did not surpass the GAM.

**Ridge Regression**, which shrinks coefficients but doesn't typically perform variable elimination, also yielded results similar to the Lasso and refined GLM models. Strong predictors for "FALSE POSITIVE" included `koi_teq`, `koi_depth`, and `koi_ror`, while `koi_incl` and `koi_smet` were significant for "CANDIDATE".

## Overall conclusion

Based on the comprehensive evaluation, the **Generalized Additive Model (GAM) better explains the phenomena** of classifying Kepler Objects of Interest. It achieved the highest predictive accuracy across all reported metrics (AUC, Accuracy, Sensitivity, and Specificity). The explicit modeling of non-linear relationships for several key astrophysical parameters allowed the GAM to capture more nuanced patterns in the data compared to the linear models (GLM, Lasso, Ridge). While the GLM with interactions attempted to capture more complexity, it suffered from instability. The Lasso and Ridge models provided regularization and, in the case of Lasso, feature selection, but their overall performance did not match that of the GAM.

The GAM's ability to flexibly model the impact of variables like equilibrium temperature (`koi_teq`), stellar surface gravity (`koi_slogg`), planet-star distance over star radius (`koi_dor`), and stellar metallicity (`koi_smet`) as non-linear effects, while retaining strong linear predictors like the planet-to-star radius ratio (`koi_ror`), provides a more robust and accurate framework for distinguishing true exoplanet candidates from false positives in this dataset.
